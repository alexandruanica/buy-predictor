{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "### Import Final Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1534, 49) (2636, 49) (1534, 1) (2636, 1)\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you have a DataFrame `df` with features `X` and a target column `y`\n",
    "X_train = pd.read_csv(\"../Final_Data/X_train.csv\")\n",
    "y_train = pd.read_csv(\"../Final_Data/y_train.csv\")\n",
    "X_test = pd.read_csv(\"../Final_Data/X_test.csv\")\n",
    "y_test = pd.read_csv(\"../Final_Data/y_test.csv\")\n",
    "\n",
    "# Now you have your train and test datasets ready\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[1;32m      3\u001b[0m categorical_columns \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcountry_United_States\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcountry_Saudi_Arabia\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcountry_Italy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m ]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Separate continuous and categorical columns\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "categorical_columns = [\n",
    "    \"country_United_States\",\n",
    "    \"country_Saudi_Arabia\",\n",
    "    \"country_India\",\n",
    "    \"country_Britain\",\n",
    "    \"country_Italy\",\n",
    "]\n",
    "\n",
    "# Separate continuous and categorical columns\n",
    "X_categorical = X_train[categorical_columns]\n",
    "X_continuous = X_train.drop(columns=categorical_columns)\n",
    "\n",
    "# Apply scaling only to continuous columns\n",
    "scaler = StandardScaler()\n",
    "X_continuous_scaled = pd.DataFrame(scaler.fit_transform(X_continuous))\n",
    "\n",
    "# Concatenate the scaled continuous columns with the unchanged categorical columns\n",
    "X_train_scaled = pd.concat(\n",
    "    [X_continuous_scaled, X_categorical.reset_index(drop=True)], axis=1\n",
    ")\n",
    "\n",
    "# Apply the same transformation to the test set\n",
    "X_categorical_test = X_test[categorical_columns]\n",
    "X_continuous_test = X_test.drop(columns=categorical_columns)\n",
    "\n",
    "X_continuous_test_scaled = pd.DataFrame(scaler.transform(X_continuous_test))\n",
    "X_test_scaled = pd.concat(\n",
    "    [X_continuous_test_scaled, X_categorical_test.reset_index(drop=True)], axis=1\n",
    ")\n",
    "\n",
    "# Ensure that all column names are strings\n",
    "X_train_scaled.columns = X_train_scaled.columns.astype(str)\n",
    "X_test_scaled.columns = X_test_scaled.columns.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different Models\n",
    "\n",
    "#### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    auc,\n",
    "    precision_recall_curve,\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    \"hidden_layer_sizes\": [(50,), (100,), (150,), (50, 50), (100, 50)],\n",
    "    \"activation\": [\"relu\", \"tanh\", \"logistic\"],\n",
    "    \"solver\": [\"adam\", \"sgd\"],\n",
    "    \"alpha\": [0.0001, 0.001, 0.01],\n",
    "    \"learning_rate\": [\"constant\", \"adaptive\"],\n",
    "}\n",
    "\n",
    "# Initialize MLPClassifier and GridSearchCV\n",
    "mlp = MLPClassifier(max_iter=500, random_state=42)\n",
    "grid_search = GridSearchCV(mlp, param_grid, cv=5, scoring=\"recall\", n_jobs=-1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Best model from grid search\n",
    "best_mlp = grid_search.best_estimator_\n",
    "\n",
    "# Predict using best model\n",
    "y_pred = best_mlp.predict(X_test_scaled)\n",
    "y_pred_prob = best_mlp.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "# Calculate PR AUC\n",
    "precision_vals, recall_vals, _ = precision_recall_curve(y_test, y_pred_prob)\n",
    "pr_auc = auc(recall_vals, precision_vals)\n",
    "\n",
    "# Create a DataFrame for metrics\n",
    "metrics_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Metric\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"ROC AUC\", \"PR AUC\"],\n",
    "        \"Value\": [accuracy, precision, recall, f1, roc_auc, pr_auc],\n",
    "    }\n",
    ")\n",
    "\n",
    "# Display the best model and metrics\n",
    "print(\"Best Model Parameters:\", grid_search.best_params_)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "conf_matrix_reversed = np.flip(conf_matrix, axis=(0, 1))\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    conf_matrix_reversed,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    cbar=False,\n",
    "    xticklabels=[\"Predicted Positive\", \"Predicted Negative\"],\n",
    "    yticklabels=[\"Actual Positive\", \"Actual Negative\"],\n",
    ")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Precision-Recall (PR) curve\n",
    "plt.figure()\n",
    "plt.plot(\n",
    "    recall_vals,\n",
    "    precision_vals,\n",
    "    color=\"orange\",\n",
    "    lw=2,\n",
    "    label=f\"PR curve (AUC = {pr_auc:.2f})\",\n",
    ")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall (PR) Curve\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Define parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    \"n_neighbors\": [3, 5, 7, 9, 11, 13],\n",
    "    \"weights\": [\"uniform\", \"distance\"],\n",
    "    \"metric\": [\"euclidean\", \"manhattan\", \"minkowski\"],\n",
    "}\n",
    "\n",
    "# Initialize KNeighborsClassifier and GridSearchCV\n",
    "knn = KNeighborsClassifier()\n",
    "grid_search = GridSearchCV(knn, param_grid, cv=5, scoring=\"recall\", n_jobs=-1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Best model from grid search\n",
    "best_knn = grid_search.best_estimator_\n",
    "\n",
    "# Predict using best model\n",
    "y_pred = best_knn.predict(X_test_scaled)\n",
    "y_pred_prob = best_knn.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "# Calculate PR AUC\n",
    "precision_vals, recall_vals, _ = precision_recall_curve(y_test, y_pred_prob)\n",
    "pr_auc = auc(recall_vals, precision_vals)\n",
    "\n",
    "# Create a DataFrame for metrics\n",
    "metrics_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Metric\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"ROC AUC\", \"PR AUC\"],\n",
    "        \"Value\": [accuracy, precision, recall, f1, roc_auc, pr_auc],\n",
    "    }\n",
    ")\n",
    "\n",
    "# Display the best model and metrics\n",
    "print(\"Best Model Parameters:\", grid_search.best_params_)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "conf_matrix_reversed = np.flip(conf_matrix, axis=(0, 1))\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    conf_matrix_reversed,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    cbar=False,\n",
    "    xticklabels=[\"Predicted Positive\", \"Predicted Negative\"],\n",
    "    yticklabels=[\"Actual Positive\", \"Actual Negative\"],\n",
    ")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Precision-Recall (PR) curve\n",
    "plt.figure()\n",
    "plt.plot(\n",
    "    recall_vals,\n",
    "    precision_vals,\n",
    "    color=\"orange\",\n",
    "    lw=2,\n",
    "    label=f\"PR curve (AUC = {pr_auc:.2f})\",\n",
    ")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall (PR) Curve\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    \"n_estimators\": [50, 100, 200],\n",
    "    \"max_depth\": [None, 10, 20, 30],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "    \"max_features\": [\"sqrt\", \"log2\"],\n",
    "}\n",
    "\n",
    "# Initialize RandomForestClassifier and GridSearchCV\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=5, scoring=\"recall\", n_jobs=-1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Best model from grid search\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "# Predict using best model\n",
    "y_pred = best_rf.predict(X_test_scaled)\n",
    "y_pred_prob = best_rf.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "# Calculate PR AUC\n",
    "precision_vals, recall_vals, _ = precision_recall_curve(y_test, y_pred_prob)\n",
    "pr_auc = auc(recall_vals, precision_vals)\n",
    "\n",
    "# Create a DataFrame for metrics\n",
    "metrics_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Metric\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"ROC AUC\", \"PR AUC\"],\n",
    "        \"Value\": [accuracy, precision, recall, f1, roc_auc, pr_auc],\n",
    "    }\n",
    ")\n",
    "\n",
    "# Display the best model and metrics\n",
    "print(\"Best Model Parameters:\", grid_search.best_params_)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "conf_matrix_reversed = np.flip(conf_matrix, axis=(0, 1))\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    conf_matrix_reversed,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    cbar=False,\n",
    "    xticklabels=[\"Predicted Positive\", \"Predicted Negative\"],\n",
    "    yticklabels=[\"Actual Positive\", \"Actual Negative\"],\n",
    ")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Precision-Recall (PR) curve\n",
    "plt.figure()\n",
    "plt.plot(\n",
    "    recall_vals,\n",
    "    precision_vals,\n",
    "    color=\"orange\",\n",
    "    lw=2,\n",
    "    label=f\"PR curve (AUC = {pr_auc:.2f})\",\n",
    ")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall (PR) Curve\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logisitic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Identify numeric and categorical columns\n",
    "numeric_cols = X_train.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "categorical_cols = X_train.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "# Use ColumnTransformer to apply OneHotEncoding to categorical columns and leave numeric columns as they are\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), numeric_cols),\n",
    "        (\n",
    "            \"cat\",\n",
    "            OneHotEncoder(handle_unknown=\"ignore\"),\n",
    "            categorical_cols,\n",
    "        ),  # Ignore unknown categories\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Apply the preprocessing pipeline to the training data\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# Apply the preprocessing pipeline to the test data\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "# Define parameter grid for Logistic Regression with L2 regularization (ridge)\n",
    "log_param_grid = {\n",
    "    \"C\": [\n",
    "        0.01,\n",
    "        0.1,\n",
    "        1,\n",
    "        10,\n",
    "        100,\n",
    "    ],  # Regularization strength (inverse of regularization term)\n",
    "    \"solver\": [\"lbfgs\", \"liblinear\"],  # Solvers that support regularization\n",
    "}\n",
    "\n",
    "# Apply GridSearchCV to find the best parameters for Logistic Regression\n",
    "log_grid_search = GridSearchCV(\n",
    "    estimator=LogisticRegression(),\n",
    "    param_grid=log_param_grid,\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "log_grid_search.fit(X_train_processed, y_train)\n",
    "\n",
    "# Get best Logistic Regression model\n",
    "best_log_model = log_grid_search.best_estimator_\n",
    "\n",
    "# Perform cross-validation on the best Logistic Regression model\n",
    "log_cv_scores = cross_val_score(\n",
    "    best_log_model, X_train_processed, y_train, cv=5, scoring=\"roc_auc\"\n",
    ")\n",
    "\n",
    "# Predict and evaluate using the tuned model\n",
    "log_y_pred = best_log_model.predict(X_test_processed)\n",
    "log_y_pred_prob = best_log_model.predict_proba(X_test_processed)[:, 1]\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, log_y_pred)\n",
    "precision = precision_score(y_test, log_y_pred)\n",
    "recall = recall_score(y_test, log_y_pred)\n",
    "f1 = f1_score(y_test, log_y_pred)\n",
    "roc_auc = roc_auc_score(y_test, log_y_pred_prob)\n",
    "\n",
    "# Calculate PR AUC\n",
    "precision_vals, recall_vals, _ = precision_recall_curve(y_test, log_y_pred_prob)\n",
    "pr_auc = auc(recall_vals, precision_vals)\n",
    "\n",
    "# Create a DataFrame for metrics\n",
    "metrics_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Metric\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"ROC AUC\", \"PR AUC\"],\n",
    "        \"Value\": [accuracy, precision, recall, f1, roc_auc, pr_auc],\n",
    "    }\n",
    ")\n",
    "\n",
    "# Display the best model and metrics\n",
    "print(\"Best Model Parameters:\", log_grid_search.best_params_)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, log_y_pred)\n",
    "conf_matrix_reversed = np.flip(conf_matrix, axis=(0, 1))\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    conf_matrix_reversed,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    cbar=False,\n",
    "    xticklabels=[\"Predicted Positive\", \"Predicted Negative\"],\n",
    "    yticklabels=[\"Actual Positive\", \"Actual Negative\"],\n",
    ")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Precision-Recall (PR) curve\n",
    "plt.figure()\n",
    "plt.plot(\n",
    "    recall_vals,\n",
    "    precision_vals,\n",
    "    color=\"orange\",\n",
    "    lw=2,\n",
    "    label=f\"PR curve (AUC = {pr_auc:.2f})\",\n",
    ")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall (PR) Curve\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Define parameter grid for Decision Tree\n",
    "tree_param_grid = {\n",
    "    \"max_depth\": [None, 10, 20, 30, 40, 50],\n",
    "    \"min_samples_split\": [2, 10, 20],\n",
    "    \"min_samples_leaf\": [1, 5, 10],\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "}\n",
    "\n",
    "# Apply GridSearchCV to find the best parameters for Decision Tree\n",
    "tree_grid_search = GridSearchCV(\n",
    "    estimator=DecisionTreeClassifier(),\n",
    "    param_grid=tree_param_grid,\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "tree_grid_search.fit(X_train_processed, y_train)\n",
    "\n",
    "# Get the best Decision Tree model\n",
    "best_tree_model = tree_grid_search.best_estimator_\n",
    "\n",
    "# Perform cross-validation on the best Decision Tree model\n",
    "tree_cv_scores = cross_val_score(\n",
    "    best_tree_model, X_train_processed, y_train, cv=5, scoring=\"roc_auc\"\n",
    ")\n",
    "\n",
    "# Predict and evaluate using the tuned Decision Tree model\n",
    "tree_y_pred = best_tree_model.predict(X_test_processed)\n",
    "tree_y_pred_prob = best_tree_model.predict_proba(X_test_processed)[:, 1]\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, tree_y_pred)\n",
    "precision = precision_score(y_test, tree_y_pred)\n",
    "recall = recall_score(y_test, tree_y_pred)\n",
    "f1 = f1_score(y_test, tree_y_pred)\n",
    "roc_auc = roc_auc_score(y_test, tree_y_pred_prob)\n",
    "\n",
    "# Calculate PR AUC\n",
    "precision_vals, recall_vals, _ = precision_recall_curve(y_test, tree_y_pred_prob)\n",
    "pr_auc = auc(recall_vals, precision_vals)\n",
    "\n",
    "# Create a DataFrame for metrics\n",
    "metrics_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Metric\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"ROC AUC\", \"PR AUC\"],\n",
    "        \"Value\": [accuracy, precision, recall, f1, roc_auc, pr_auc],\n",
    "    }\n",
    ")\n",
    "\n",
    "# Display the best model and metrics\n",
    "print(\"Best Model Parameters:\", tree_grid_search.best_params_)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, tree_y_pred)\n",
    "conf_matrix_reversed = np.flip(conf_matrix, axis=(0, 1))\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    conf_matrix_reversed,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    cbar=False,\n",
    "    xticklabels=[\"Predicted Positive\", \"Predicted Negative\"],\n",
    "    yticklabels=[\"Actual Positive\", \"Actual Negative\"],\n",
    ")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Precision-Recall (PR) curve\n",
    "plt.figure()\n",
    "plt.plot(\n",
    "    recall_vals,\n",
    "    precision_vals,\n",
    "    color=\"orange\",\n",
    "    lw=2,\n",
    "    label=f\"PR curve (AUC = {pr_auc:.2f})\",\n",
    ")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall (PR) Curve\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate\n",
    "from sklearn.metrics import (\n",
    "    make_scorer,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    \"n_estimators\": [50, 100, 150],\n",
    "    \"learning_rate\": [0.05, 0.1, 0.2],\n",
    "    \"max_depth\": [3, 5, 7],\n",
    "}\n",
    "\n",
    "# Initialize the Gradient Boosting model\n",
    "model = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Define scoring metrics for cross-validation\n",
    "scoring = {\n",
    "    \"accuracy\": make_scorer(accuracy_score),\n",
    "    \"precision\": make_scorer(precision_score),\n",
    "    \"recall\": make_scorer(recall_score),\n",
    "    \"roc_auc\": make_scorer(roc_auc_score, needs_proba=True),\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV with cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"recall\",\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    ")\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best model from grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Perform cross-validation on the best model\n",
    "cv_results = cross_validate(\n",
    "    best_model, X_train_scaled, y_train, cv=5, scoring=scoring, return_train_score=False\n",
    ")\n",
    "\n",
    "# Calculate and print mean cross-validated metrics\n",
    "accuracy_cv = np.mean(cv_results[\"test_accuracy\"])\n",
    "precision_cv = np.mean(cv_results[\"test_precision\"])\n",
    "recall_cv = np.mean(cv_results[\"test_recall\"])\n",
    "roc_auc_cv = np.mean(cv_results[\"test_roc_auc\"])\n",
    "\n",
    "print(f\"\\nCross-Validated Metrics for Best Model:\")\n",
    "print(f\"Accuracy: {accuracy_cv:.4f}\")\n",
    "print(f\"Precision: {precision_cv:.4f}\")\n",
    "print(f\"Recall: {recall_cv:.4f}\")\n",
    "print(f\"ROC AUC: {roc_auc_cv:.4f}\")\n",
    "\n",
    "# Fit the best model on the full training data and evaluate on test set\n",
    "best_model.fit(X_train_scaled, y_train)\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "y_pred_prob = best_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Calculate metrics on test set\n",
    "accuracy_test = accuracy_score(y_test, y_pred)\n",
    "precision_test = precision_score(y_test, y_pred)\n",
    "recall_test = recall_score(y_test, y_pred)\n",
    "roc_auc_test = roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "# Confusion matrix for test set\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "conf_matrix_reversed = np.flip(conf_matrix, axis=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "conf_matrix_reversed = np.flip(conf_matrix, axis=(0, 1))\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "# Calculate PR AUC\n",
    "precision_vals, recall_vals, _ = precision_recall_curve(y_test, y_pred_prob)\n",
    "pr_auc = auc(recall_vals, precision_vals)\n",
    "\n",
    "# Create a DataFrame for metrics\n",
    "metrics_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Metric\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"ROC AUC\", \"PR AUC\"],\n",
    "        \"Value\": [accuracy, precision, recall, f1, roc_auc, pr_auc],\n",
    "    }\n",
    ")\n",
    "\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    conf_matrix_reversed,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    cbar=False,\n",
    "    xticklabels=[\"Predicted Positive\", \"Predicted Negative\"],\n",
    "    yticklabels=[\"Actual Positive\", \"Actual Negative\"],\n",
    ")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Precision-Recall (PR) curve\n",
    "plt.figure()\n",
    "plt.plot(\n",
    "    recall_vals,\n",
    "    precision_vals,\n",
    "    color=\"orange\",\n",
    "    lw=2,\n",
    "    label=f\"PR curve (AUC = {pr_auc:.2f})\",\n",
    ")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall (PR) Curve\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate\n",
    "from sklearn.metrics import (\n",
    "    make_scorer,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    ")\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the parameter grid with regularization hyperparameters\n",
    "param_grid = {\n",
    "    \"n_estimators\": [50, 100, 150],\n",
    "    \"learning_rate\": [0.05, 0.1, 0.2],\n",
    "    \"max_depth\": [3, 5, 7],\n",
    "    \"subsample\": [0.8, 1.0],  # Regularization by subsampling\n",
    "    \"min_samples_split\": [2, 5, 10],  # Minimum samples required to split a node\n",
    "    \"min_samples_leaf\": [1, 2, 4],  # Minimum samples required at each leaf node\n",
    "    \"max_features\": [\n",
    "        \"sqrt\",\n",
    "        \"log2\",\n",
    "        None,\n",
    "    ],  # Maximum number of features considered for each split\n",
    "}\n",
    "\n",
    "# Initialize the Gradient Boosting model\n",
    "model = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Define scoring metrics for cross-validation\n",
    "scoring = {\n",
    "    \"accuracy\": make_scorer(accuracy_score),\n",
    "    \"precision\": make_scorer(precision_score),\n",
    "    \"recall\": make_scorer(recall_score),\n",
    "    \"roc_auc\": make_scorer(roc_auc_score, needs_proba=True),\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV with cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"recall\",\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model from grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Perform cross-validation on the best model\n",
    "cv_results = cross_validate(\n",
    "    best_model, X_train, y_train, cv=5, scoring=scoring, return_train_score=False\n",
    ")\n",
    "\n",
    "# Calculate and print mean cross-validated metrics\n",
    "accuracy_cv = np.mean(cv_results[\"test_accuracy\"])\n",
    "precision_cv = np.mean(cv_results[\"test_precision\"])\n",
    "recall_cv = np.mean(cv_results[\"test_recall\"])\n",
    "roc_auc_cv = np.mean(cv_results[\"test_roc_auc\"])\n",
    "\n",
    "print(f\"\\nCross-Validated Metrics for Best Model:\")\n",
    "print(f\"Accuracy: {accuracy_cv:.4f}\")\n",
    "print(f\"Precision: {precision_cv:.4f}\")\n",
    "print(f\"Recall: {recall_cv:.4f}\")\n",
    "print(f\"ROC AUC: {roc_auc_cv:.4f}\")\n",
    "\n",
    "# Fit the best model on the full training data and evaluate on test set\n",
    "best_model.fit(X_train, y_train)\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate metrics on test set\n",
    "accuracy_test = accuracy_score(y_test, y_pred)\n",
    "precision_test = precision_score(y_test, y_pred)\n",
    "recall_test = recall_score(y_test, y_pred)\n",
    "roc_auc_test = roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "# Confusion matrix for test set\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "conf_matrix_reversed = np.flip(conf_matrix, axis=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "conf_matrix_reversed = np.flip(conf_matrix, axis=(0, 1))\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "# Calculate PR AUC\n",
    "precision_vals, recall_vals, _ = precision_recall_curve(y_test, y_pred_prob)\n",
    "pr_auc = auc(recall_vals, precision_vals)\n",
    "\n",
    "# Create a DataFrame for metrics\n",
    "metrics_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Metric\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"ROC AUC\", \"PR AUC\"],\n",
    "        \"Value\": [accuracy, precision, recall, f1, roc_auc, pr_auc],\n",
    "    }\n",
    ")\n",
    "\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    conf_matrix_reversed,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    cbar=False,\n",
    "    xticklabels=[\"Predicted Positive\", \"Predicted Negative\"],\n",
    "    yticklabels=[\"Actual Positive\", \"Actual Negative\"],\n",
    ")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Precision-Recall (PR) curve\n",
    "plt.figure()\n",
    "plt.plot(\n",
    "    recall_vals,\n",
    "    precision_vals,\n",
    "    color=\"orange\",\n",
    "    lw=2,\n",
    "    label=f\"PR curve (AUC = {pr_auc:.2f})\",\n",
    ")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall (PR) Curve\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SHAP is a method of local explanation that tells us how much each feature contributes to the model.\n",
    "\n",
    "The larger the bar, the larger the contribution for the feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure you use TreeExplainer for GradientBoostingClassifier\n",
    "explainer = shap.TreeExplainer(best_model)\n",
    "\n",
    "# Compute SHAP values\n",
    "shap_values = explainer.shap_values(X_train)\n",
    "\n",
    "# Convert SHAP values to DataFrame for analysis\n",
    "shap_df = pd.DataFrame(shap_values, columns=X_train.columns)\n",
    "\n",
    "# Identify the top 5 features by mean absolute SHAP value\n",
    "top_features = shap_df.abs().mean().nlargest(5).index\n",
    "\n",
    "# Filter data for the top features\n",
    "X_train_top = X_train[top_features]\n",
    "\n",
    "# Summary plot (bar chart) for top features\n",
    "shap.summary_plot(shap_values, X_train, plot_type=\"bar\", max_display=5, show=False)\n",
    "plt.title(\"Top 5 Features - SHAP Feature Importance\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Beeswarm plot for the top 5 features\n",
    "shap.summary_plot(shap_values[:, : len(top_features)], X_train_top, show=False)\n",
    "plt.title(\"Top 5 Features - SHAP Impact on Predictions\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selected Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export predicted probabilities\n",
    "y_pred_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "y_pred_prob = pd.DataFrame(y_pred_prob, columns=[\"predicted_probability\"])\n",
    "y_pred_prob.to_csv(\"predicted_probabilities.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the model\n",
    "with open(\"best_model.pkl\", \"wb\") as file:\n",
    "    pickle.dump(best_model, file)\n",
    "print(\"Model saved to best_model.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
